---
layout: post
title: Review_SPnet:Estimating Garment Sewing Patterns from a Single Image
subtitle: There's lots to learn!
comments: true
mathjax: true
author: Suah Yu
---

## 포즈를 취한 사용자의 단일 이미지에서 3D 의복 모델을 재구성하는 새로운 방법을 제시


### ◆ 이전의 연구:
 입력된 의복 이미지와 일치하도록 의복 기하학적 구조를 정확하게 재구성 
    –> 한계 : 새로운 자세를 위해 변형될 때 의복이 부자연스러워 보임.
### ◆ 본 연구:
* 단일 이미지에서 **제봉 패턴**을 통해 **의복의 기본 모양**을 추론(3D 의복을 바로 재구성하지 X)
    -> 기대효과: 포즈가 변형되어도 의류가 부자연스러워 보이지 않음.
* SPnet이라는 딥러닝을 통해 의복의 제봉 패턴을 예측한다.
* 주름같은 변형이 문제점
    ->T-pose로 의류 형상을 예측한 후 재봉 패턴을 예측하는 두 단계 접근법으로 정확도 향상

### ◆ 단계 요약:
1. T-pose garment predictor : 포즈를 취한 사용자의 단일 이미지가 주어지면, (옷의 기본 형태를 나타내는) T-pose에 착용한 옷 이미지를 예측한다. **포즈의 영향에서 벗어나 의복의 original 모양을 추론** 한다. 즉, t-pose 옷 예측기가 임의 포즈 input image를 t-pose의 옷모양으로 변환한다.
2. sewing pattern parameter predictor : 얻은 T-pose 의복 의미지 기반의 **제봉 패턴 parameter**를 측정한다. 제봉 스티칭을 물리학적으로 시뮬레이션하고, 3d 의복 모델을 생성한다.
->예측된 parameter로부터 생성된 제봉 패턴을 3d body model에 시뮬레이션함으로써, 자연스러운 3d 의복을 얻는다.

### ◆ T-pose garment predictor
1. 입력 이미지(Is)는 옷, 얼굴,머리 정보를 포함하는데, 의복 추론을 위해 source image에서 필요한 정보만 추출.
    1) 사용자의 포즈를 나타내는 pose map(Ps) 
    2) posed된 의복의 모양을 나타내는 garment normal map(Gs)
두 가지만 추출하여 네트워크에 입력한다.
2. 의복의 target pose image로 Pt가 제공됨.
3. 네트워크는 Pt와 추출한 map 정보를 바탕으로 **T-pose 의류 이미지 Gt를 예측**한다.

* T-pose garment predictor 훈련 방법:
L1 손실 함수를 사용하여 예측된 T-포즈 의류 이미지와 실제 T-포즈 의류 이미지 간의 차이를 최소화한다.


손실 함수: LT=∣Gt−Gtgt∣
*Gt는 예측된 T-포즈 의류 이미지, Gtgt는 실제 T-포즈 의류 이미지

### ◆ sewing pattern parameter predictor
1. 목표 : T-pose garment image, gt로부터 sewing pattern parameter st를 예측
2. 방법 :
    1) t-pose 의류 이미지를 입력으로 받아, 정보를 latent vector로 인코딩한다.
    2) 인코딩 된 정보를 바탕으로 MLP layer를 통해 재봉 패턴 파라미터를 예측한다.

* 네트워크의 구성:Convolutional Autoencoder와 Multilayer Perceptron(MLP) 구조의 결합
1) 입력 data : t-pose 의류 이미지
2) **Convolutional Autoencoder**
2-1. encoder : t-pose 의류 이미지의 중요한 정보를 latent vector로 인코딩
2-2. decoder : 벡터를 다시 원래 이미지 크기로 복원. 재구성
3) **MLP**
인코딩 정보를 바탕으로 재봉 패턴 파라미터 예측
*예측하는 재봉 패턴 parameter이 포함하는 것:의류의 길이, 너비, 깊이
각 의복 타입마다 재봉 패턴 파라미터가 다르므로, 타입마다 개별적인 훈련을 거쳐야 함.
*sewing pattern parameter predictor 훈련 방법:
L1 손실 함수를 사용하여 예측된 파라미터와 실제 파라미터 간의 차이를 최소화한다.


손실 함수: Lsewing=∣St−Stgt∣
*St는 예측된 재봉 패턴 파라미터, Stgt는 실제 재봉 패턴 파라미터
*latent vector : T-pose 의류 이미지의 형태와 관련된 압축된 정보를 나타냄.

### ◆ dataset
훈련 데이터 :　다양한 의복과 포즈를 가진 이미지, t-pose 의류 이미지, 재봉 패턴 파라미터

1) simulate garments on the t-pose SMPL avatar using Qualoth software(다양한 의류를 다양한 체형과 포즈를 가진 가상 아바타 모델에 입힌다.)
2) 가상 아바타 모델을 T-pose로 설정해서 의류의 기본 형태를 캡쳐한다.
3) render resulting garment using Arnold renderer.

----

#### Use
학습:가상 아바타에게 의류를 입히고, 다양한 포즈를 설정해서 3D 의복 모델을 얻는다.
->학습을 통해 얻어진 데이터로, 실제 사람의 이미지에 적용해서 3d 의복 모델 얻기 가능

#### 궁금한 점 
1. input이 단일 이미지로 주어지는데 어떤 방법을 통해 ps와 gs로 분리할 수 있는지(각 맵을 어떻게 생성하는지)
2. 특징적 의류 유형에 따라 패턴 파라미터를 설정하는데, 슬리브가 뒤쪽 면만 있고 앞쪽은 파여있는 옷 같은 경우(따로 물리적 특성을 지니지 않음)를 적용했을 때에도 자연스럽게 표현이 가능한지

#### 특징
장점 : 사용자의 다양한 체형과 포즈를 반영한다는 점
단점 : SPnet의 모델은 주로 의류의 주름, 변형을 예측하는 데 최적화되있다. 주머니, 지퍼, 후드와 같은 세부 요소는 의류의 기본적인 패턴과 달리 추가적이고 디테일한 물리적 특성을 가졌기에 구조적 정보를 필요로 한다. 
(ex)주머니는 의류 표면에서 돌출되어 있음. 지퍼는 특정한 움직임을 가짐. 후드는 뒤쪽에 추가적인 무게를 추가함.+주머니는 사용자의 손과 상호작용 가능, 후드는 사용자의 머리와 상호작용 가능