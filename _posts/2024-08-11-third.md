---
layout: post
title: Work Summary (1)
subtitle: There's lots to learn!
gh-repo: daattali/beautiful-jekyll
comments: true
mathjax: true
author: Suah Yu
---

### <포즈를 취한 사용자의 단일 이미지에서 3D 의복 모델을 재구성하는 새로운 방법을 제시>


#### ◆ 이전의 연구

#### ◆ 본 연구
1. 목표 :
 source garment의 detail이 반영되며, target garment의 general한 shape을 따라가는 output 예측 및 생성

2. 기대 효과 :


#### ◆ cage network 요약
1. input
1-1) source garment : T-pose 3D scan data, real garment in test time
1-2) target garment : Posed 3D mesh data, animatable simple garment

2. Cage Net & Deform Net





-----


<source cage가 잘못 예측되는 원인 추론>

model의 point들을 고려할 cage의 vertex가 부족하다보니 cage 밖으로 model이 extrude됨.


3. cshape loss 구하는 코드
<사진>
model의 가장자리(구멍 뚫린 부분)에는 triangle mesh가 존재하지 않아서,
구멍 방향 normal이 존재하지 않음.
<사진>
-normal이 존재하는 model의 두께는 fit하게 맞춰지지만, 구멍 부분에 관여하는 model의 길이에는 fit하게 맞춰지지 않고, extrude됨.
-ref_surface의 구멍 뚫린 부분은 원본과 동일하므로,
source cage가 생성되는 과정에서 model의 구멍 뚫린 부분은 고려하지 않은 채로 mvc loss를 적용해 model을 감싸려고 하다보니 구멍 뚫린 부분이 과도하게 줄어들음.

수정 방안 : 
1) source_shape을 scale해서 키운 ref_surface의 y축 최대지점을 1.005로 설정하여 ref_surface가 모든 size의 model을 전부 cover할 수 있도록 함.
2) 구멍 부분에도 normal을 생성하는 방법

<deformed cage 및 output이 잘못 예측되는 원인 추론>
*source cage가 잘 예측되어, model을 fit하게 bind&cover한다고 가정

cage 생성하는 코드 : networks.py > def forward 함수

-함수의 input인 source_shape (B,3,N)와 target_shape (B,3,M)
-x,y,z 위치정보만 담고있는 3 dimension임.
-x,y,z 각각의 vertex normal 정보를 추가해 6 dimension을 전달.


chamfer distance의 value를 1보다 더 키우면 loss가 증가해 학습이 잘 안됨.
-> shape preservation하는 다른 loss 찾기


deform_with_GC코드 : source shape과 cage 사이에서 찾은 coords_v와 coords_f를 deformed cage에 적용시켜 output을 얻는 과정
=output에 coords_f도 고려되어 적용되었으므로, 내뱉는 weight값에도 coords_f가 고려되어야 한다.


<defense 예상 질문>
1. ref_surface(source shape + 0.1 * source normal)에서 sampling한 것을 cage로 사용하면 더 좋지 않을까?
-> sampling 방법에 따라 결과가 달라짐.
-> input size가 다양한데, 각 다양한 size마다 중요한 부분을 동일하게 sampling하기 어려움.




*Gt는 예측된 T-포즈 의류 이미지, Gtgt는 실제 T-포즈 의류 이미지

#### ◆ sewing pattern parameter predictor
1. 목표 : T-pose garment image, gt로부터 sewing pattern parameter st를 예측


*St는 예측된 재봉 패턴 파라미터, Stgt는 실제 재봉 패턴 파라미터

*latent vector : T-pose 의류 이미지의 형태와 관련된 압축된 정보를 나타냄.

### ◆ dataset
훈련 데이터 :　다양한 의복과 포즈를 가진 이미지, t-pose 의류 이미지, 재봉 패턴 파라미터


----

#### Use
학습:가상 아바타에게 의류를 입히고, 다양한 포즈를 설정해서 3D 의복 모델을 얻는다.
->학습을 통해 얻어진 데이터로, 실제 사람의 이미지에 적용해서 3d 의복 모델 얻기 가능

#### 궁금한 점 
1. input이 단일 이미지로 주어지는데 어떤 방법을 통해 ps와 gs로 분리할 수 있는지(각 맵을 어떻게 생성하는지)
2. 특징적 의류 유형에 따라 패턴 파라미터를 설정하는데, 슬리브가 뒤쪽 면만 있고 앞쪽은 파여있는 옷 같은 경우(따로 물리적 특성을 지니지 않음)를 적용했을 때에도 자연스럽게 표현이 가능한지

#### 특징
장점 : 사용자의 다양한 체형과 포즈를 반영한다는 점

단점 : SPnet의 모델은 주로 의류의 주름, 변형을 예측하는 데 최적화되있다. 주머니, 지퍼, 후드와 같은 세부 요소는 의류의 기본적인 패턴과 달리 추가적이고 디테일한 물리적 특성을 가졌기에 구조적 정보를 필요로 한다. 

(ex)주머니는 의류 표면에서 돌출되어 있음. 지퍼는 특정한 움직임을 가짐. 후드는 뒤쪽에 추가적인 무게를 추가함.+주머니는 사용자의 손과 상호작용 가능, 후드는 사용자의 머리와 상호작용 가능